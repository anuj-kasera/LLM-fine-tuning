{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ken5e2GibXni"
   },
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwAJBKLBZVbw"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMGb6gVjZTP-"
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count trainable and total parameters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    # Loop through all model parameters\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()           # total params\n",
    "        if param.requires_grad:                    # check if trainable\n",
    "            trainable_model_params += param.numel()\n",
    "\n",
    "\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n\" \\\n",
    "           f\"all model parameters: {all_model_params}\\n\" \\\n",
    "           f\"percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKmdrdpuZyl7"
   },
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test sample by index\n",
    "index = 200\n",
    "\n",
    "# Extract dialogue and human-written summary\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Create summarization prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt into model inputs\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate summary using the model\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,     # limit output length\n",
    "    )[0],\n",
    "    skip_special_tokens=True   # clean special tokens\n",
    ")\n",
    "\n",
    "# Formatting line for readability\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "# Print input prompt, human summary, and model output\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p45Ou3cLZ81-"
   },
   "source": [
    "# Full Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize dialogue-summary pairs\n",
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    # Build input prompt for each dialogue\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "\n",
    "    # Tokenize inputs (prompts) and labels (summaries)\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply tokenization to all dataset splits (train/val/test)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unused columns, keeping only tokenized data\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first tokenized input (prompt + dialogue)\n",
    "sample_input_id = tokenized_datasets['train']['input_ids'][0:1]\n",
    "\n",
    "# Take first tokenized label (summary)\n",
    "sample_label = tokenized_datasets['train']['labels'][0:1]\n",
    "\n",
    "# Print dataset object details\n",
    "print(\"tokenized_datasets: \", tokenized_datasets)\n",
    "\n",
    "# Print sample input ID length and values\n",
    "print(\"\\nsample_input_id: \", len(sample_input_id[0]), sample_input_id)\n",
    "\n",
    "# Print sample label length and values\n",
    "print(\"\\nsample_label: \", len(sample_label[0]), sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset shapes (rows, columns) for each split\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "# Print tokenized dataset object summary\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO0Uszzna-bt"
   },
   "source": [
    "## Fine-Tune the Model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique output directory using current timestamp\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# Define training hyperparameters and logging settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",   # <-- correct name in your version\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer with model, args, and datasets\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fully fine-tuned model\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load true original (pretrained) for baseline comparison\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-base\", torch_dtype=\"bfloat16\"\n",
    ").to(device).eval()\n",
    "\n",
    "# Load your fine-tuned model\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    output_dir, torch_dtype=\"bfloat16\", local_files_only=True\n",
    ").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick test sample\n",
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Create summarization prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate summary with original model (zero-shot)\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate summary with fine-tuned instruct model\n",
    "instruct_model_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print human, original, and instruct model summaries\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AQIPhFjvSxj"
   },
   "source": [
    "## Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Select first 10 test dialogues and human summaries\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# Initialize lists to store model outputs\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "# Generate summaries for each dialogue\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Original model summary\n",
    "    original_model_outputs = original_model.generate(\n",
    "        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    # Fine-tuned instruct model summary\n",
    "    instruct_model_outputs = instruct_model.generate(\n",
    "        input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)\n",
    "    )\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "# Combine human and model summaries for comparison\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores for the original model\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,  # aggregate scores across all examples\n",
    "    use_stemmer=True      # apply stemming for better matching\n",
    ")\n",
    "\n",
    "# Compute ROUGE scores for the fine-tuned instruct model\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "# Print ROUGE evaluation results\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZlM933OlbNL"
   },
   "source": [
    "# Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,        #\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the original model\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAK4UPpAmHIv"
   },
   "source": [
    "## Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Training arguments (Seq2Seq version)\n",
    "peft_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",   # <-- correct name in your version\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize PEFT trainer\n",
    "peft_trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\"validation\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start LoRA fine-tuning\n",
    "peft_trainer.train()\n",
    "\n",
    "# Define path to save the trained model\n",
    "peft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load pre-trained base model and tokenizer\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-base\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Load LoRA-adapted PEFT model from checkpoint, frozen for inference\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    './peft-dialogue-summary-checkpoint-local',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agLvJ8bEoBqm"
   },
   "source": [
    "### Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to device\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load true original (pretrained) for baseline comparison\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-base\", torch_dtype=\"bfloat16\"\n",
    ").to(device).eval()\n",
    "\n",
    "# Load your fine-tuned model\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    output_dir, torch_dtype=\"bfloat16\", local_files_only=True\n",
    ").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# Prepare summarization prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "# Tokenize prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "# Generate summary using original model\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate summary using instruction-tuned model\n",
    "instruct_model_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate summary using PEFT (LoRA) model\n",
    "peft_model_outputs = peft_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print comparison\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{peft_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjZnmMtdoFLe"
   },
   "source": [
    "###  Evaluate the Model Quantitatively (with ROUGE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select first 10 test dialogues and their human summaries\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "# Lists to store model outputs\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "# Generate summaries for each dialogue\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "\n",
    "    # Generate outputs for all models\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append outputs to lists\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "# Combine human and model summaries into a DataFrame\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute ROUGE for original model\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# Compute ROUGE for instruction-tuned model\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# Compute ROUGE for PEFT model\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
